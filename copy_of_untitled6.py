# -*- coding: utf-8 -*-
"""Copy of Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_nlc2IHqKYTO3x33fFxybPHjqncYTu3H
"""

from huggingface_hub import login
login()

# âœ… Step 1: Install required libraries
!pip install -q --upgrade datasets huggingface_hub

# âœ… Step 2: Import necessary libraries
from datasets import load_dataset

# âœ… Step 3: Load the mini-peS2o dataset with authentication
print("ðŸ“¥ Loading mini-peS2o dataset...")
# Changed use_auth_token=True to token=True
dataset = load_dataset("nampdn-ai/mini-peS2o", split="train", token=True)

# âœ… Step 4: Reduce to ~50k samples to keep it ~500MB
dataset = dataset.select(range(50000))

print(f"âœ… Loaded {len(dataset)} samples.")

# âœ… Step 1: Install required libraries (if not already done)
!pip install -q --upgrade transformers accelerate

# âœ… Step 2: Import libraries
from transformers import (
    PhiConfig,
    PhiForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
)
import torch
import numpy as np

# âœ… Step 3: Load tokenizer (from Phi-2)
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2", trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… Step 4: Tokenize your mini-peS2o dataset
def tokenize_function(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=128,
        return_special_tokens_mask=True
    )

print("ðŸ”§ Tokenizing mini-peS2o dataset...")
tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# âœ… Step 5: Define model config from scratch
config = PhiConfig(
    vocab_size=len(tokenizer),   # Important to prevent index errors
    num_hidden_layers=6,         # Adjust based on memory (4â€“12 layers)
    num_attention_heads=4,
    hidden_size=256,
    intermediate_size=1024,
    max_position_embeddings=128
)

# âœ… Step 6: Initialize model from scratch
model = PhiForCausalLM(config)

# âœ… Step 7: Define data collator for CLM
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # For Causal LM
)

# âœ… Step 8: Define training arguments
training_args = TrainingArguments(
    output_dir="./phi2-mini-pretrained",
    overwrite_output_dir=True,
    num_train_epochs=3,  # Adjust for longer training
    per_device_train_batch_size=8,
    save_steps=500,
    logging_steps=100,
    eval_strategy="no",
    report_to="none",
    fp16=torch.cuda.is_available()
)

# âœ… Step 9: Define a dummy accuracy metric (for illustration)
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    preds = np.argmax(predictions, axis=-1)
    mask = labels != -100
    correct = (preds == labels) & mask
    accuracy = correct.sum() / mask.sum()
    return {"accuracy": accuracy}

# âœ… Step 10: Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# âœ… Step 11: Start training
print("ðŸš€ Pretraining your Phi-style model on mini-peS2o...")
trainer.train()

pip install --upgrade transformers

import shutil

shutil.make_archive('/content/phi2-mini-pretrained', 'zip', '/content/phi2-mini-pretrained')

from google.colab import files

files.download('/content/phi2-mini-pretrained.zip')